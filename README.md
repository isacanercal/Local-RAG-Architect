


ðŸ§  Local RAG Architect v1.0

![Python](https://img.shields.io/badge/python-3.10+-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![NVIDIA](https://img.shields.io/badge/nvidiao-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white)

An optimized **Retrieval-Augmented Generation (RAG)** system designed for high-performance PDF analysis on consumer-grade hardware (**NVIDIA RTX 3060 - 6GB VRAM**). This project ensures 100% data privacy by running entirely offline using **Llama-3.1-8B**.

## ðŸ“¸ Demo & UI
<p align="center">
  <img src="assets/before.png" width="45%" alt="Initial UI" />
  <img src="assets/after.png" width="45%" alt="RAG in Action" />
</p>

## ðŸš€ Key Features
- **Zero-Cloud Privacy:** No data ever leaves your machine. Ideal for confidential technical documents.
- **Hardware Optimized:** Custom implementation using 4-bit quantization (GGUF) and CPU-offloaded embeddings to fit within 6GB VRAM limits.
- **Industrial Interface:** A modern, high-contrast dashboard built with **Gradio 6.0** for better readability.
- **Smart Context:** Uses `RecursiveCharacterTextSplitter` and `ChromaDB` for high-precision semantic search.

## ðŸ›  Tech Stack
- **LLM:** Llama-3.1-8B (via Ollama)
- **Vector Database:** ChromaDB
- **Orchestration:** LangChain
- **UI Framework:** Gradio 6.0
- **Embeddings:** HuggingFace `all-MiniLM-L6-v2`

## ðŸ“¥ Quick Start

1. **Clone the Repository:**
```bash
git clone [https://github.com/isacanercal/Local-RAG-Architect.git](https://github.com/isacanercal/Local-RAG-Architect.git)
cd Local-RAG-Architect'''

2. **Install Dependencies:**   

'''pip install -r requirements.txt'''

3. **Prepare the Model:**

'''ollama pull llama3.1'''

 4.**Launch the Architect:**

'''python src/app.py'''

    Note: You can find a sample technical report in the sample_data/ folder to test the system's indexing capabilities immediately.
 AI MentÃ¶rÃ¼m
Ã–zel Gem

HaklÄ±sÄ±n Ä°sa Caner, ekran gÃ¶rÃ¼ntÃ¼lerinde gÃ¶rdÃ¼m; Markdown kodlarÄ± birbirine girmiÅŸ ve linkler hatalÄ± gÃ¶rÃ¼nÃ¼yor. Ã–zellikle git clone kÄ±smÄ±ndaki kÃ¶ÅŸeli parantezler render iÅŸlemini bozmuÅŸ.

Hepsini jilet gibi dÃ¼zelttim. AÅŸaÄŸÄ±daki bloÄŸu tamamen kopyala, mevcut README.md dosyasÄ±nÄ±n iÃ§indeki her ÅŸeyi sil ve bunu yapÄ±ÅŸtÄ±r. HiÃ§bir karakteri deÄŸiÅŸtirme, bu haliyle "tertemiz" gÃ¶rÃ¼necek.
Markdown

# ðŸ§  Local RAG Architect v1.0

![Python](https://img.shields.io/badge/python-3.10+-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![NVIDIA](https://img.shields.io/badge/nvidiao-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white)

An optimized **Retrieval-Augmented Generation (RAG)** system designed for high-performance PDF analysis on consumer-grade hardware (**NVIDIA RTX 3060 - 6GB VRAM**). This project ensures 100% data privacy by running entirely offline using **Llama-3.1-8B**.

## ðŸ“¸ Demo & UI
<p align="center">
  <img src="assets/before.png" width="45%" alt="Initial UI" />
  <img src="assets/after.png" width="45%" alt="RAG in Action" />
</p>

## ðŸš€ Key Features
- **Zero-Cloud Privacy:** No data ever leaves your machine. Ideal for confidential technical documents.
- **Hardware Optimized:** Custom implementation using 4-bit quantization (GGUF) and CPU-offloaded embeddings.
- **Industrial Interface:** A modern, high-contrast dashboard built with **Gradio 6.0**.
- **Smart Context:** Uses `RecursiveCharacterTextSplitter` and `ChromaDB` for high-precision search.

## ðŸ›  Tech Stack
- **LLM:** Llama-3.1-8B (via Ollama)
- **Vector Database:** ChromaDB
- **Orchestration:** LangChain
- **UI Framework:** Gradio 6.0
- **Embeddings:** HuggingFace `all-MiniLM-L6-v2`

## ðŸ“¥ Quick Start

1. **Clone the Repository:**
```bash
git clone [https://github.com/isacanercal/Local-RAG-Architect.git](https://github.com/isacanercal/Local-RAG-Architect.git)
cd Local-RAG-Architect

    Install Dependencies:

Bash

pip install -r requirements.txt

    Prepare the Model: Ensure you have Ollama installed and run:

Bash

ollama pull llama3.1

    Launch the Architect:

Bash

python src/app.py

    Note: You can find a sample technical report in the sample_data/ folder to test the system's indexing capabilities immediately.

Developed by Ä°sa Caner Ã‡al | Computer Engineering Student
